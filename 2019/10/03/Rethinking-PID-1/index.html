<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  <title>Rethinking PID 1 | What Did I Just Read</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <link rel="shortcut icon" href="/">
  <link rel="stylesheet" href="/css/app.css">
  <!-- <link rel='stylesheet' href='http://fonts.useso.com/css?family=Source+Code+Pro'> -->
  
</head>
</html>
<body>
  <nav class="app-nav">
  
    
      <a href="/.">home</a>
    
  
    
      <a href="/archives">archive</a>
    
  
    
      <a href="/about">about</a>
    
  
</nav>

  <main class="post">
  <article>
  <h1 class="article-title">
    <a href="/2019/10/03/Rethinking-PID-1/">Rethinking PID 1</a>
  </h1>

  <section class="article-meta">
    <p class="article-date">October 02 2019</p>
  </section>

  <section class="article-entry">
    <blockquote>
<p>Here’s the link to the original post written by Lennart Poettering:<br><a href="http://0pointer.net/blog/projects/systemd.html" target="_blank" rel="noopener">http://0pointer.net/blog/projects/systemd.html</a></p>
</blockquote>
<p>If you are well connected or good at reading between the lines you might already know what this blog post is about. But even then you may find this story interesting. So grab a cup of coffee, sit down, and read what’s coming.</p>
<p>This blog story is long, so even though I can only recommend reading the long story, here’s the one sentence summary: we are experimenting with a new init system and it is fun.</p>
<p><a href="http://git.0pointer.de/?p=systemd.git" target="_blank" rel="noopener">Here’s the code.</a> And here’s the story:</p>
<h4 id="Process-Identifier-1"><a href="#Process-Identifier-1" class="headerlink" title="Process Identifier 1"></a>Process Identifier 1</h4><p>On every Unix system there is one process with the special process identifier 1. It is started by the kernel before all other processes and is the parent process for all those other processes that have nobody else to be child of. Due to that it can do a lot of stuff that other processes cannot do. And it is also responsible for some things that other processes are not responsible for, such as bringing up and maintaining userspace during boot.</p>
<a id="more"></a>

<p>Historically on Linux the software acting as PID 1 was the venerable sysvinit package, though it had been showing its age for quite a while. Many replacements have been suggested, only one of them really took off: <a href="http://upstart.ubuntu.com/" target="_blank" rel="noopener">Upstart</a>, which has by now found its way into all major distributions.</p>
<p>As mentioned, the central responsibility of an init system is to bring up userspace. And a good init system does that fast. Unfortunately, the traditional SysV init system was not particularly fast.</p>
<p>For a fast and efficient boot-up two things are crucial:</p>
<ul>
<li>To start <strong>less</strong>.</li>
<li>And to start <strong>more</strong> in <em>parallel</em>.</li>
</ul>
<p>What does that mean? Starting less means starting fewer services or deferring the starting of services until they are actually needed. There are some services where we know that they will be required sooner or later (syslog, D-Bus system bus, etc.), but for many others this isn’t the case. For example, bluetoothd does not need to be running unless a bluetooth dongle is actually plugged in or an application wants to talk to its D-Bus interfaces. Same for a printing system: unless the machine physically is connected to a printer, or an application wants to print something, there is no need to run a printing daemon such as CUPS. Avahi: if the machine is not connected to a network, there is no need to run <a href="http://avahi.org/" target="_blank" rel="noopener">Avahi</a>, unless some application wants to use its APIs. And even SSH: as long as nobody wants to contact your machine there is no need to run it, as long as it is then started on the first connection. (And admit it, on most machines where sshd might be listening somebody connects to it only every other month or so.)</p>
<p>Starting more in parallel means that if we have to run something, we should not serialize its start-up (as sysvinit does), but run it all at the same time, so that the available CPU and disk IO bandwidth is maxed out, and hence the overall start-up time minimized.</p>
<h4 id="Hardware-and-Software-Change-Dynamically"><a href="#Hardware-and-Software-Change-Dynamically" class="headerlink" title="Hardware and Software Change Dynamically"></a>Hardware and Software Change Dynamically</h4><p>Modern systems (especially general purpose OS) are highly dynamic in their configuration and use: they are mobile, different applications are started and stopped, different hardware added and removed again. An init system that is responsible for maintaining services needs to listen to hardware and software changes. It needs to dynamically start (and sometimes stop) services as they are needed to run a program or enable some hardware.</p>
<p>Most current systems that try to parallelize boot-up still synchronize the start-up of the various daemons involved: since Avahi needs D-Bus, D-Bus is started first, and only when D-Bus signals that it is ready, Avahi is started too. Similar for other services: livirtd and X11 need HAL (well, I am considering the Fedora 13 services here, ignore that HAL is obsolete), hence HAL is started first, before livirtd and X11 are started. And libvirtd also needs Avahi, so it waits for Avahi too. And all of them require syslog, so they all wait until Syslog is fully started up and initialized. And so on.</p>
<h4 id="Parallelizing-Socket-Services"><a href="#Parallelizing-Socket-Services" class="headerlink" title="Parallelizing Socket Services"></a>Parallelizing Socket Services</h4><p>This kind of start-up synchronization results in the serialization of a significant part of the boot process. Wouldn’t it be great if we could get rid of the synchronization and serialization cost? Well, we can, actually. For that, we need to understand what exactly the daemons require from each other, and why their start-up is delayed. For traditional Unix daemons, there’s one answer to it: they wait until the socket the other daemon offers its services on is ready for connections. Usually that is an AF_UNIX socket in the file-system, but it could be AF_INET[6], too. For example, clients of D-Bus wait that <code>/var/run/dbus/system_bus_socket</code> can be connected to, clients of syslog wait for <code>/dev/log</code>, clients of CUPS wait for <code>/var/run/cups/cups.sock</code> and NFS mounts wait for <code>/var/run/rpcbind.sock</code> and the portmapper IP port, and so on. And think about it, this is actually the only thing they wait for!</p>
<p>Now, if that’s all they are waiting for, if we manage to make those sockets available for connection earlier and only actually wait for that instead of the full daemon start-up, then we can speed up the entire boot and start more processes in parallel. So, how can we do that? Actually quite easily in Unix-like systems: we can create the listening sockets <strong>before</strong> we actually start the daemon, and then just pass the socket during <code>exec()</code> to it. That way, we can create <strong>all</strong> sockets for <strong>all</strong> daemons in one step in the init system, and then in a second step run all daemons at once. If a service needs another, and it is not fully started up, that’s completely OK: what will happen is that the connection is queued in the providing service and the client will potentially block on that single request. But only that one client will block and only on that one request. Also, dependencies between services will no longer necessarily have to be configured to allow proper parallelized start-up: if we start all sockets at once and a service needs another it can be sure that it can connect to its socket.</p>
<p>Because this is at the core of what is following, let me say this again, with different words and by example: if you start syslog and and various syslog clients at the same time, what will happen in the scheme pointed out above is that the messages of the clients will be added to the <code>/dev/log</code> socket buffer. As long as that buffer doesn’t run full, the clients will not have to wait in any way and can immediately proceed with their start-up. As soon as syslog itself finished start-up, it will dequeue all messages and process them. Another example: we start D-Bus and several clients at the same time. If a synchronous bus request is sent and hence a reply expected, what will happen is that the client will have to block, however only that one client and only until D-Bus managed to catch up and process it.</p>
<p>Basically, the kernel socket buffers help us to maximize parallelization, and the ordering and synchronization is done by the kernel, without any further management from userspace! And if all the sockets are available before the daemons actually start-up, dependency management also becomes redundant (or at least secondary): if a daemon needs another daemon, it will just connect to it. If the other daemon is already started, this will immediately succeed. If it isn’t started but in the process of being started, the first daemon will not even have to wait for it, unless it issues a synchronous request. And even if the other daemon is not running at all, it can be auto-spawned. From the first daemon’s perspective there is no difference, hence dependency management becomes mostly unnecessary or at least secondary, and all of this in optimal parallelization and optionally with on-demand loading. On top of this, this is also more robust, because the sockets stay available regardless whether the actual daemons might temporarily become unavailable (maybe due to crashing). In fact, you can easily write a daemon with this that can run, and exit (or crash), and run again and exit again (and so on), and all of that without the clients noticing or loosing any request.</p>
<p>It’s a good time for a pause, go and refill your coffee mug, and be assured, there is more interesting stuff following.</p>
<p>But first, let’s clear a few things up: is this kind of logic new? No, it certainly is not. The most prominent system that works like this is Apple’s launchd system: on MacOS the listening of the sockets is pulled out of all daemons and done by launchd. The services themselves hence can all start up in parallel and dependencies need not to be configured for them. And that is actually a really ingenious design, and the primary reason why MacOS manages to provide the fantastic boot-up times it provides. I can highly recommend <a href="https://www.youtube.com/watch?v=SjrtySM9Dns" target="_blank" rel="noopener">this video</a> where the launchd folks explain what they are doing. Unfortunately this idea never really took on outside of the Apple camp.</p>
<p>The idea is actually even older than launchd. Prior to launchd the venerable <code>inetd</code> worked much like this: sockets were centrally created in a daemon that would start the actual service daemons passing the socket file descriptors during <code>exec()</code>. However the focus of <code>inetd</code> certainly wasn’t local services, but Internet services (although later reimplementations supported AF_UNIX sockets, too). It also wasn’t a tool to parallelize boot-up or even useful for getting implicit dependencies right.</p>
<p>For TCP sockets <code>inetd</code> was primarily used in a way that for every incoming connection a new daemon instance was spawned. That meant that for each connection a new process was spawned and initialized, which is not a recipe for high-performance servers. However, right from the beginning <code>inetd</code> also supported another mode, where a single daemon was spawned on the first connection, and that single instance would then go on and also accept the follow-up connections (that’s what the <code>wait</code>/<code>nowait</code> option in <code>inetd.conf</code> was for, a particularly badly documented option, unfortunately.) Per-connection daemon starts probably gave inetd its bad reputation for being slow. But that’s not entirely fair.</p>
<h4 id="Parallelizing-Bus-Services"><a href="#Parallelizing-Bus-Services" class="headerlink" title="Parallelizing Bus Services"></a>Parallelizing Bus Services</h4><p>Modern daemons on Linux tend to provide services via D-Bus instead of plain AF_UNIX sockets. Now, the question is, for those services, can we apply the same parallelizing boot logic as for traditional socket services? Yes, we can, D-Bus already has all the right hooks for it: using bus activation a service can be started the first time it is accessed. Bus activation also gives us the minimal per-request synchronisation we need for starting up the providers and the consumers of D-Bus services at the same time: if we want to start Avahi at the same time as CUPS (side note: CUPS uses Avahi to browse for mDNS/DNS-SD printers), then we can simply run them at the same time, and if CUPS is quicker than Avahi via the bus activation logic we can get D-Bus to queue the request until Avahi manages to establish its service name.</p>
<p>So, in summary: the socket-based service activation and the bus-based service activation together enable us to start <strong>all</strong> daemons in parallel, without any further synchronization. Activation also allows us to do lazy-loading of services: if a service is rarely used, we can just load it the first time somebody accesses the socket or bus name, instead of starting it during boot.</p>
<p>And if that’s not great, then I don’t <strong>know</strong> what is great!</p>
<h4 id="Parallelizing-File-System-Jobs"><a href="#Parallelizing-File-System-Jobs" class="headerlink" title="Parallelizing File System Jobs"></a>Parallelizing File System Jobs</h4><p>If you look <a href="http://picasaweb.google.com/betsubetsu43/Fedora#5179125455943690130" target="_blank" rel="noopener">at the serialization graphs of the boot process</a> of current distributions, there are more synchronisation points than just daemon start-ups: most prominently there are file-system related jobs: mounting, fscking, quota. Right now, on boot-up a lot of time is spent idling to wait until all devices that are listed in <code>/etc/fstab</code> show up in the device tree and are then fsck’ed, mounted, quota checked (if enabled). Only after that is fully finished we go on and boot the actual services.</p>
<p>Can we improve this? It turns out we can. Harald Hoyer came up with the idea of using the venerable autofs system for this:</p>
<p>Just like a <code>connect()</code> call shows that a service is interested in another service, an <code>open()</code> (or a similar call) shows that a service is interested in a specific file or file-system. So, in order to improve how much we can parallelize we can make those apps wait only if a file-system they are looking for is not yet mounted and readily available: we set up an autofs mount point, and then when our file-system finished fsck and quota due to normal boot-up we replace it by the real mount. While the file-system is not ready yet, the access will be queued by the kernel and the accessing process will block, but only that one daemon and only that one access. And this way we can begin starting our daemons even before all file systems have been fully made available – without them missing any files, and maximizing parallelization.</p>
<p>Parallelizing file system jobs and service jobs does not make sense for <code>/</code>, after all that’s where the service binaries are usually stored. However, for file-systems such as <code>/home</code>, that usually are bigger, even encrypted, possibly remote and seldom accessed by the usual boot-up daemons, this can improve boot time considerably. It is probably not necessary to mention this, but virtual file systems, such as procfs or sysfs should never be mounted via autofs.</p>
<p>I wouldn’t be surprised if some readers might find integrating autofs in an init system a bit fragile and even weird, and maybe more on the “crackish” side of things. However, having played around with this extensively I can tell you that this actually feels quite right. Using autofs here simply means that we can create a mount point without having to provide the backing file system right-away. In effect it hence only delays accesses. If an application tries to access an autofs file-system and we take very long to replace it with the real file-system, it will hang in an interruptible sleep, meaning that you can safely cancel it, for example via C-c. Also note that at any point, if the mount point should not be mountable in the end (maybe because fsck failed), we can just tell autofs to return a clean error code (like ENOENT). So, I guess what I want to say is that even though integrating autofs into an init system might appear adventurous at first, our experimental code has shown that this idea works surprisingly well in practice – if it is done for the right reasons and the right way.</p>
<p>Also note that these should be <em>direct</em> autofs mounts, meaning that from an application perspective there’s little effective difference between a classic mount point and one based on autofs.</p>
<h4 id="Keeping-the-First-User-PID-Small"><a href="#Keeping-the-First-User-PID-Small" class="headerlink" title="Keeping the First User PID Small"></a>Keeping the First User PID Small</h4><p>Another thing we can learn from the MacOS boot-up logic is that shell scripts are evil. Shell is fast and shell is slow. It is fast to hack, but slow in execution. The classic sysvinit boot logic is modelled around shell scripts. Whether it is <code>/bin/bash</code> or any other shell (that was written to make shell scripts faster), in the end the approach is doomed to be slow. On my system the scripts in <code>/etc/init.d</code> call <code>grep</code> at least 77 times. <code>awk</code> is called 92 times, <code>cut</code> 23 and <code>sed</code> 74. Every time those commands (and others) are called, a process is spawned, the libraries searched, some start-up stuff like i18n and so on set up and more. And then after seldom doing more than a trivial string operation the process is terminated again. Of course, that has to be incredibly slow. No other language but shell would do something like that. On top of that, shell scripts are also very fragile, and change their behaviour drastically based on environment variables and suchlike, stuff that is hard to oversee and control.</p>
<p>So, let’s get rid of shell scripts in the boot process! Before we can do that we need to figure out what they are currently actually used for: well, the big picture is that most of the time, what they do is actually quite boring. Most of the scripting is spent on trivial setup and tear-down of services, and should be rewritten in C, either in separate executables, or moved into the daemons themselves, or simply be done in the init system.</p>
<p>It is not likely that we can get rid of shell scripts during system boot-up entirely anytime soon. Rewriting them in C takes time, in a few case does not really make sense, and sometimes shell scripts are just too handy to do without. But we can certainly make them less prominent.</p>
<p>A good metric for measuring shell script infestation of the boot process is the PID number of the first process you can start after the system is fully booted up. Boot up, log in, open a terminal, and type <code>echo $$</code>. Try that on your Linux system, and then compare the result with MacOS! (Hint, it’s something like this: Linux PID 1823; MacOS PID 154, measured on test systems we own.)</p>
<h4 id="Keeping-Track-of-Processes"><a href="#Keeping-Track-of-Processes" class="headerlink" title="Keeping Track of Processes"></a>Keeping Track of Processes</h4><p>A central part of a system that starts up and maintains services should be process babysitting: it should watch services. Restart them if they shut down. If they crash it should collect information about them, and keep it around for the administrator, and cross-link that information with what is available from crash dump systems such as abrt, and in logging systems like syslog or the audit system.</p>
<p>It should also be capable of shutting down a service completely. That might sound easy, but is harder than you think. Traditionally on Unix a process that does double-forking can escape the supervision of its parent, and the old parent will not learn about the relation of the new process to the one it actually started. An example: currently, a misbehaving CGI script that has double-forked is not terminated when you shut down Apache. Furthermore, you will not even be able to figure out its relation to Apache, unless you know it by name and purpose.</p>
<p>So, how can we keep track of processes, so that they cannot escape the babysitter, and that we can control them as one unit even if they fork a gazillion times?</p>
<p>Different people came up with different solutions for this. I am not going into much detail here, but let’s at least say that approaches based on ptrace or the netlink connector (a kernel interface which allows you to get a netlink message each time any process on the system fork()s or exit()s) that some people have investigated and implemented, have been criticised as ugly and not very scalable.</p>
<p>So what can we do about this? Well, since quite a while the kernel knows <a href="http://git.kernel.org/gitweb.cgi?p=linux/kernel/git/torvalds/linux-2.6.git;a=blob;f=Documentation/cgroups/cgroups.txt;hb=HEAD" target="_blank" rel="noopener">Control Groups</a> (aka “cgroups”). Basically they allow the creation of a hierarchy of groups of processes. The hierarchy is directly exposed in a virtual file-system, and hence easily accessible. The group names are basically directory names in that file-system. If a process belonging to a specific cgroup fork()s, its child will become a member of the same group. Unless it is privileged and has access to the cgroup file system it cannot escape its group. Originally, cgroups have been introduced into the kernel for the purpose of containers: certain kernel subsystems can enforce limits on resources of certain groups, such as limiting CPU or memory usage. Traditional resource limits (as implemented by <code>setrlimit()</code>) are (mostly) per-process. cgroups on the other hand let you enforce limits on entire groups of processes. cgroups are also useful to enforce limits outside of the immediate container use case. You can use it for example to limit the total amount of memory or CPU Apache and all its children may use. Then, a misbehaving CGI script can no longer escape your <code>setrlimit()</code> resource control by simply forking away.</p>
<p>In addition to container and resource limit enforcement cgroups are very useful to keep track of daemons: cgroup membership is securely inherited by child processes, they cannot escape. There’s a notification system available so that a supervisor process can be notified when a cgroup runs empty. You can find the cgroups of a process by reading <code>/proc/$PID/cgroup</code>. cgroups hence make a very good choice to keep track of processes for babysitting purposes.</p>
<h4 id="Controlling-the-Process-Execution-Environment"><a href="#Controlling-the-Process-Execution-Environment" class="headerlink" title="Controlling the Process Execution Environment"></a>Controlling the Process Execution Environment</h4><p>A good babysitter should not only oversee and control when a daemon starts, ends or crashes, but also set up a good, minimal, and secure working environment for it.</p>
<p>That means setting obvious process parameters such as the <code>setrlimit()</code> resource limits, user/group IDs or the environment block, but does not end there. The Linux kernel gives users and administrators a lot of control over processes (some of it is rarely used, currently). For each process you can set CPU and IO scheduler controls, the capability bounding set, CPU affinity or of course cgroup environments with additional limits, and more.</p>
<p>As an example, <code>ioprio_set()</code> with <code>IOPRIO_CLASS_IDLE</code> is a great away to minimize the effect of <code>locate</code>‘s <code>updatedb</code> on system interactivity.</p>
<p>On top of that certain high-level controls can be very useful, such as setting up read-only file system overlays based on read-only bind mounts. That way one can run certain daemons so that all (or some) file systems appear read-only to them, so that EROFS is returned on every write request. As such this can be used to lock down what daemons can do similar in fashion to a poor man’s SELinux policy system (but this certainly doesn’t replace SELinux, don’t get any bad ideas, please).</p>
<p>Finally logging is an important part of executing services: ideally every bit of output a service generates should be logged away. An init system should hence provide logging to daemons it spawns right from the beginning, and connect stdout and stderr to syslog or in some cases even <code>/dev/kmsg</code> which in many cases makes a very useful replacement for syslog (embedded folks, listen up!), especially in times where the kernel log buffer is configured ridiculously large out-of-the-box.</p>
<h4 id="On-Upstart"><a href="#On-Upstart" class="headerlink" title="On Upstart"></a>On Upstart</h4><p>To begin with, let me emphasize that I actually like the code of Upstart, it is very well commented and easy to follow. It’s certainly something other projects should learn from (including my own).</p>
<p>That being said, I can’t say I agree with the general approach of Upstart. But first, a bit more about the project:</p>
<p>Upstart does not share code with sysvinit, and its functionality is a super-set of it, and provides compatibility to some degree with the well known SysV init scripts. It’s main feature is its event-based approach: starting and stopping of processes is bound to “events” happening in the system, where an “event” can be a lot of different things, such as: a network interfaces becomes available or some other software has been started.</p>
<p>Upstart does service serialization via these events: if the <code>syslog-started</code> event is triggered this is used as an indication to start D-Bus since it can now make use of Syslog. And then, when <code>dbus-started</code> is triggered, <code>NetworkManager</code> is started, since it may now use <code>D-Bus</code>, and so on.</p>
<p>One could say that this way the actual logical dependency tree that exists and is understood by the admin or developer is translated and encoded into event and action rules: every logical “a needs b” rule that the administrator/developer is aware of becomes a “start a when b is started” plus “stop a when b is stopped”. In some way this certainly is a simplification: especially for the code in Upstart itself. However I would argue that this simplification is actually detrimental. First of all, the logical dependency system does not go away, the person who is writing Upstart files must now translate the dependencies manually into these event/action rules (actually, two rules for each dependency). So, instead of letting the computer figure out what to do based on the dependencies, the user has to manually translate the dependencies into simple event/action rules. Also, because the dependency information has never been encoded it is not available at runtime, effectively meaning that an administrator who tries to figure our <em>why</em> something happened, i.e. why a is started when b is started, has no chance of finding that out.</p>
<p>Furthermore, the event logic turns around all dependencies, from the feet onto their head. Instead of <em>minimizing</em> the amount of work (which is something that a good init system should focus on, as pointed out in the beginning of this blog story), it actually <em>maximizes</em> the amount of work to do during operations. Or in other words, instead of having a clear goal and only doing the things it really needs to do to reach the goal, it does one step, and then after finishing it, it does <strong>all</strong> steps that possibly could follow it.</p>
<p>Or to put it simpler: the fact that the user just started D-Bus is in no way an indication that NetworkManager should be started too (but this is what Upstart would do). It’s right the other way round: when the user asks for NetworkManager, that is definitely an indication that D-Bus should be started too (which is certainly what most users would expect, right?).</p>
<p>A good init system should start only what is needed, and that on-demand. Either lazily or parallelized and in advance. However it should not start more than necessary, particularly not everything installed that could use that service.</p>
<p>Finally, I fail to see the actual usefulness of the event logic. It appears to me that most events that are exposed in Upstart actually are not punctual in nature, but have duration: a service starts, is running, and stops. A device is plugged in, is available, and is plugged out again. A mount point is in the process of being mounted, is fully mounted, or is being unmounted. A power plug is plugged in, the system runs on AC, and the power plug is pulled. Only a minority of the events an init system or process supervisor should handle are actually punctual, most of them are tuples of start, condition, and stop. This information is again not available in Upstart, because it focuses in singular events, and ignores durable dependencies.</p>
<p>Now, I am aware that some of the issues I pointed out above are in some way mitigated by certain more recent changes in Upstart, particularly condition based syntaxes such as <code>start on (local-filesystems and net-device-up IFACE=lo)</code> in Upstart rule files. However, to me this appears mostly as an attempt to fix a system whose core design is flawed.</p>
<p>Besides that Upstart does OK for babysitting daemons, even though some choices might be questionable (see above), and there are certainly a lot of missed opportunities (see above, too).</p>
<p>There are other init systems besides sysvinit, Upstart and launchd. Most of them offer little substantial more than Upstart or sysvinit. The most interesting other contender is Solaris SMF, which supports proper dependencies between services. However, in many ways it is overly complex and, let’s say, a bit <em>academic</em> with its excessive use of XML and new terminology for known things. It is also closely bound to Solaris specific features such as the <em>contract</em> system.</p>
<h4 id="Putting-it-All-Together-systemd"><a href="#Putting-it-All-Together-systemd" class="headerlink" title="Putting it All Together: systemd"></a>Putting it All Together: systemd</h4><p>Well, this is another good time for a little pause, because after I have hopefully explained above what I think a good PID 1 should be doing and what the current most used system does, we’ll now come to where the beef is. So, go and refill you coffee mug again. It’s going to be worth it.</p>
<p>You probably guessed it: what I suggested above as requirements and features for an ideal init system is actually available now, in a (still experimental) init system called <code>systemd</code>, and which I hereby want to announce. <a href="http://git.0pointer.de/?p=systemd.git" target="_blank" rel="noopener">Again, here’s the code.</a> And here’s a quick rundown of its features, and the rationale behind them:</p>
<p>systemd starts up and supervises the entire system (hence the name…). It implements all of the features pointed out above and a few more. It is based around the notion of <em>units</em>. Units have a name and a type. Since their configuration is usually loaded directly from the file system, these unit names are actually file names. Example: a unit <code>avahi.service</code> is read from a configuration file by the same name, and of course could be a unit encapsulating the Avahi daemon. There are several kinds of units:</p>
<ol>
<li><code>service</code>: these are the most obvious kind of unit: daemons that can be started, stopped, restarted, reloaded. For compatibility with SysV we not only support our own configuration files for services, but also are able to read classic SysV init scripts, in particular we parse the LSB header, if it exists. <code>/etc/init.d</code> is hence not much more than just another source of configuration.</li>
<li><code>socket</code>: this unit encapsulates a socket in the file-system or on the Internet. We currently support AF_INET, AF_INET6, AF_UNIX sockets of the types stream, datagram, and sequential packet. We also support classic FIFOs as transport. Each <code>socket</code> unit has a matching <code>service</code> unit, that is started if the first connection comes in on the socket or FIFO. Example: <code>nscd.socket</code> starts <code>nscd.service</code> on an incoming connection.</li>
<li><code>device</code>: this unit encapsulates a device in the Linux device tree. If a device is marked for this via udev rules, it will be exposed as a <code>device</code> unit in systemd. Properties set with <code>udev</code> can be used as configuration source to set dependencies for device units.</li>
<li><code>mount</code>: this unit encapsulates a mount point in the file system hierarchy. systemd monitors all mount points how they come and go, and can also be used to mount or unmount mount-points. <code>/etc/fstab</code> is used here as an additional configuration source for these mount points, similar to how SysV init scripts can be used as additional configuration source for <code>service</code> units.</li>
<li><code>automount</code>: this unit type encapsulates an automount point in the file system hierarchy. Each <code>automount</code> unit has a matching <code>mount</code> unit, which is started (i.e. mounted) as soon as the automount directory is accessed.</li>
<li><code>target</code>: this unit type is used for logical grouping of units: instead of actually doing anything by itself it simply references other units, which thereby can be controlled together. Examples for this are: <code>multi-user.target</code>, which is a target that basically plays the role of run-level 5 on classic SysV system, or <code>bluetooth.target</code> which is requested as soon as a bluetooth dongle becomes available and which simply pulls in bluetooth related services that otherwise would not need to be started: <code>bluetoothd</code> and <code>obexd</code> and suchlike.</li>
<li><code>snapshot</code>: similar to <code>target</code> units snapshots do not actually do anything themselves and their only purpose is to reference other units. Snapshots can be used to save/rollback the state of all services and units of the init system. Primarily it has two intended use cases: to allow the user to temporarily enter a specific state such as “Emergency Shell”, terminating current services, and provide an easy way to return to the state before, pulling up all services again that got temporarily pulled down. And to ease support for system suspending: still many services cannot correctly deal with system suspend, and it is often a better idea to shut them down before suspend, and restore them afterwards.</li>
</ol>
<p>All these units can have dependencies between each other (both positive and negative, i.e. ‘Requires’ and ‘Conflicts’): a device can have a dependency on a service, meaning that as soon as a device becomes available a certain service is started. Mounts get an implicit dependency on the device they are mounted from. Mounts also gets implicit dependencies to mounts that are their prefixes (i.e. a mount <code>/home/lennart</code> implicitly gets a dependency added to the mount for <code>/home</code>) and so on.</p>

  </section>
</article>


  
</main>

</body>
</html>
